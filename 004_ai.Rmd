# Part 4 -- AI for Toxicity Prediction

In this part we experiment on using Deep Learning and more classical machine learning techniques to classsify compounds, or get a prediction for the LD50 outcome. We use the same dataset as in part 3, derived from the TAME toolbox. 
Deep Learning is a modern predictive approach where neural networks are used to train a model on labelled data. The network are able to detect pattern associated to the outcome, based on the label. There are many different architectures available to choose from. Depending on the intended task, one or more of these architectures can be chosen. Also, there are a number of implementations available to build neural networks. Popular frameworks are [PyTorch](https://pytorch.org/), [Keras](https://keras.io/) and [Tensorflow](https://www.tensorflow.org/?gclid=EAIaIQobChMIptjygcrG-wIVyOZ3Ch1omQ4MEAAYASAAEgLtF_D_BwE). PyTorch is available for Python only, whereas Keras and Tensorflow are avaible for both Python and R programming environments. Because R is relatively much used by the academic community, we use tensorflow and keras in R for this workshop. The [`{reticulate}` R package](), which is an interface to Python from R, makes it possible to setup Tensorflow in RStudio. 

It can however be quite tricky to get this technically setup, especially when you are on a new Mac with M1-chip. Luckily there is are many good resources to help you along:

 - https://developer.apple.com/metal/tensorflow-plugin/
 - https://tensorflow.rstudio.com/install/
 - https://stackoverflow.com/questions/50145643/unable-to-change-python-path-in-reticulate

For natural languge processing [Hugging Face](https://huggingface.co/) models are a good place to start.

I you are looking for a good resource to start on Deep Learning, see the excellent book from Manning that [is avaialble for R](https://www.manning.com/books/deep-learning-with-r) and for [Python](https://www.manning.com/books/deep-learning-with-python-second-edition)

## Installing Tensorflow for R
We follow the steps in theese [docs ](https://tensorflow.rstudio.com/install/) for installing Tensorflow on Windows computers. If you experience technical difficulties, we can try to help you during the workshop. We are also available after the workshop or via an online meeting to help you along if needed.

```{r include = FALSE, eval = TRUE}
# set CSS for objects
knitr::opts_chunk$set(
  class.source="Rchunk", 
  class.output="Rout", 
  warning = FALSE,
  error = FALSE,
  message = FALSE)
```

## Packages
```{r}
library(tensorflow)
library(tidyverse)
library(keras)
```

```{r include=FALSE}
load("course_urls.RData")
les <- 4
```

## Data
```{r}
substances <- read_csv(
  here::here(
    "data-raw",
    "substances.csv")) |>
  janitor::clean_names()

acute_data <- read_csv(
    here::here(
      "data-raw",
      "acute_tox_data.csv")) |>
  janitor::clean_names()

```

## Remove duplicated SMILES
Because we want to predict toxicity (class and continuous outcome of LD50_LM), we need to solve the issue of duplicated SMILES. Some smiles will have a label 'nontoxic', where that same smile will have a label 'toxic' when connected to a different compound. This can be problematic, because it will lead to ambiguous label - to - feature correlations.
To solve this, I decided to just remove all observations from the data that have a duplicated SMILES. 
```{r}
ind <- duplicated(substances$qsar_ready_smiles)
sum(ind)  

substances[ind,] -> x
acute_data[ind,] -> y

y |> group_by(nontoxic) |> tally()

## we remove the duplicates
substances <- substances[!ind, ]
acute_data <- acute_data[!ind, ]
```


## Preprocessing the data
Here we combine the data to contain the fingerprints, the `dtxsid` ids and the `nontoxic` and the `ld40_lm` column

```{r}
acute_data_select <- acute_data |>
  dplyr::select(
    dtxsid,
    nontoxic,
    ld50_lm
  )

substances_select <- substances |>
  dplyr::select(
    dtxsid,
    qsar_ready_smiles
  ) 

data_nn <- full_join(acute_data_select, substances_select)
```

We reproduce part of the previous section to convert the qsar ready smiles to fingerprints

```{r}
library(rcdk)
library(rcdklibs)
all_smiles <- substances_select$qsar_ready_smiles
all_mols <-parse.smiles(all_smiles)
all_mols[[1]]
```

### Computing chemical fingerprints

We can run the same function over the entire 'all_mols' dataset, leveraging the `map()` function from the `{purrr}` R package of the `{tidyverse}`:
```{r}
all.fp <- 
  map(all_mols, 
      get.fingerprint, 
      type='standard')

## Convert the pf list to a df
fp_tbl <- fingerprint::fp.to.matrix(all.fp) |> as_tibble()
## adding the predicted class (nontoxic as column)

fp_tbl <- fp_tbl |>
  mutate(
    class = ifelse(
      test = acute_data_select$nontoxic == "TRUE",  ## recode the class, 0 = nontoxic, 1 = toxic
      yes = 0,
      no = 1),
    ld50_lm = acute_data_select$ld50_lm) |>
  relocate(class, ld50_lm)

```

## Split data into training and test set
```{r}
library(rsample)

## seed for reproducibility
set.seed(123)
data_split <- initial_split(fp_tbl, prop = 3/4)

## trainig data
training_data <- training(data_split) |> 
  select(-c(class, ld50_lm)) |> 
  as.matrix() |>
  array_reshape(c(nrow(training(data_split)), 1*1024))

training_labels_class <- training(data_split) |> select(class) |> 
  as.matrix() |>
#  as.integer() |>
  array_reshape(nrow(training(data_split)))

## test data
test_data <- testing(data_split) |>
  select(-c(class, ld50_lm)) |> 
  as.matrix() |>
  array_reshape(c(nrow(testing(data_split)), 1*1024))

test_labels_class <- testing(data_split) |> select(class) |> 
  as.matrix() |>
# as.integer() |>
  array_reshape(nrow(testing(data_split)))
  
training_data[1,c(1:80)]
training_labels_class[1:10]
test_data[1,c(1:80)]
test_labels_class[1:10]
```

## Neural Network for binary classification
Here we use the fingerprints as 1D tensors, one tensor per compound
```{r}
set.seed(123)
model <- keras_model_sequential(input_shape = c(1*1024)) |>
  layer_dense(units = 1024, activation = "relu") %>%
  layer_dropout(rate = 0.5) |>
  layer_dense(units = 512, activation = "relu") |>
  layer_dense(units = 512, activation = "relu") |>
  layer_dropout(0.2) %>%
  layer_dense(units = 16, activation = "relu") |>
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)

## store training in a history object so that we can see how the model is doing, also on a small validation set.
## validation split, means that 20% of the training data is reserved for validation.
history <- model %>% 
  fit(training_data, 
      training_labels_class,
      epochs = 50,
      verbose = 2,
  validation_split = 0.2
)

## we can plot the history
plot(history)

## evaluate our model on thetest data
model %>% evaluate(test_data, test_labels_class, verbose = 2)

# Confusion matrix
pred <- model %>% predict(test_data, batch_size = 10)
y_pred = round(pred)
confusion_matrix = table(y_pred, test_labels_class)
confusion_matrix
```

## Optimizing the model
The model above is quite complex and big in terms of layers and number of neurons. When we look at the model on the validation set we can see in the plot that the model is overfitting. Let's start by reducing the size and complexity of the model first.
```{r}
set.seed(123)
## simpler models
model <- keras_model_sequential(input_shape = c(1*1024)) |>
  layer_dense(units = 512, activation = "relu") %>%
  layer_dropout(rate = 0.9) |>
  layer_dense(units = 512, activation = "relu") %>%
  layer_dropout(rate = 0.1) |>
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)

## store training in a history object so that we can see how the model is doing, also on a small validation set.
## validation split, means that 20% of the training data is reserved for validation.
history <- model %>% 
  fit(training_data, 
      training_labels_class,
      epochs = 20,
      # Suppress logging.
      verbose = 2,
  # Calculate validation results on 20% of the training data.
  validation_split = 0.2
)

## we can plot the history
plot(history)

## evaluate our model on thetest data
model %>% evaluate(test_data, test_labels_class, verbose = 2)

# Confusion matrix
pred <- model %>% predict(test_data, batch_size = 10)
y_pred = round(pred)
confusion_matrix = table(y_pred, test_labels_class)
confusion_matrix
```

## Prediction of a continuous outcome (regression)
Here we will try to predict the LD50_LM score, on the basis of the chemical fingerprints as tensors.
Because this outcome is on a continuous scale, we need a slightly different model architecture. This type of modelling approach is considered a regression problem.
We will repeat some of the preprocessing steps, because we need a different outcome variable.

```{r}
## continuous outcome (training)
training_labels_ld50_lm <- training(data_split) |> 
  select(ld50_lm) |>
  as.matrix() |>
  array_reshape(nrow(training(data_split)))

## continuous outcome (testing)
test_labels_ld50_lm <- testing(data_split) |> select(ld50_lm) |> 
  as.matrix() |>
  array_reshape(nrow(testing(data_split)))
  
training_labels_ld50_lm[1:10]
test_labels_ld50_lm[1:10]
```

## Regression outcome prediction
We use the same model as above but with a different output layer. Also we specify a different outcome: MAE = Mean Absolute Error, and should be as low as possible.
```{r}
set.seed(123)
model <- keras_model_sequential(input_shape = c(1*1024)) |>
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.2) |>
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.1) |>
#  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1)

model %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("mae")
)

history <- model %>% 
  fit(training_data, 
      training_labels_ld50_lm,
      epochs = 20,
      # Suppress logging.
      verbose = 2,
  # Calculate validation results on 20% of the training data.
  validation_split = 0.2
)

plot(history)

model %>% evaluate(test_data,  test_labels_ld50_lm, verbose = 2)
model |> predict(test_data) -> predictions

predictions[1,]
realvalue <- test_labels_ld50_lm[1]
realvalue

## lets plot the correlations
predictions <- tibble(
  .pred = predictions,
  .real = test_labels_ld50_lm
)

origin <- tibble(
  x = seq(-3, 5, length.out = 9),
  y = seq(-3, 5, length.out = 9)
)

predictions |>
  ggplot(aes(x = .real, y = .pred)) +
  geom_point(shape = 1) +
  geom_line(data = origin, aes(x = x, y = y), colour = "darkred")

```

## Maybe we can do better when we use different embeddings
Transformer foundation models have revolutionized Deep Learning over the past three years. They were first created and used to solve complex Natural Language Processing tasks. Now they are also used to create better representations for compounds, DNA and even for meta-genomics. Here we use the [ChemBERT model available from Hugging Face](https://huggingface.co/jiangg/chembert_cased) to create alternative structural embedding for our compounds, based on the 'QSAR ready SMILES'.
The dataset containing the embeddings was too big to put in Github. It can be found and manually downloaded from a shared Google Drive: https://drive.google.com/file/d/1RX8hd4YZ37TDpDM095XcjS8oRuOZPFc_/view?usp=share_link 

Download the file manually and upload it into the './data-raw' folder. The file is zipped via the .zip method. We need to unpack the file, before we can use it.

The data file with ChemBERTa embeddings was created using a python script which can be found in `./ChemBERT/chembert.py`. This folder also includes a `requirements.txt` file that can be used to create a reproducible Python environment. See e.g. [this resource](https://www.folkstalk.com/2022/09/conda-create-environment-based-on-requirements-txt-with-code-examples.html) on how to do that.
```{r}
file <- here::here("data-raw", "aspis_workshop_chembert_embeddings.csv")

chembert <- read_csv(
  file
) |> janitor::clean_names()

## remove duplicated smiels rows
chembert <- chembert[!ind, ]
## let's add the class and the ld50_lm to this data
chembert <- chembert |>
  mutate(
    class = fp_tbl$class,
    ld50_lm = fp_tbl$ld50_lm
  ) |>
  relocate(class, ld50_lm)

```

## Preprocessing for Neural Network
Now that we have the data with new embeddings, let's preprocess this data. We have seen how to do this before.
```{r}
library(rsample)

## seed for reproducibility
set.seed(123)
data_split <- initial_split(chembert, prop = 3/4)

## trainig data
training_data <- training(data_split) |> 
  select(-c(dtxsid, class, ld50_lm)) |> 
  as.matrix() |>
  array_reshape(c(nrow(training(data_split)), 1*768))

training_labels_class <- training(data_split) |> select(class) |> 
  as.matrix() |>
#  as.integer() |>
  array_reshape(c(nrow(training(data_split))))

## test data
test_data <- testing(data_split) |>
  select(-c(dtxsid, class, ld50_lm)) |> 
  as.matrix() |>
  array_reshape(c(nrow(testing(data_split)), 1*768))

test_labels_class <- testing(data_split) |> select(class) |> 
  as.matrix() |>
# as.integer() |>
  array_reshape(c(nrow(testing(data_split))))
  
training_data[1,c(1:80)]
training_labels_class[1:10]
test_data[1,c(1:80)]
test_labels_class[1:10]
```

## Neural Network for binary classification
Here we use the chembert embeddings as 1D tensors, one tensor per compound
```{r}
set.seed(123)
## simpler models
model <- keras_model_sequential(input_shape = c(1*768)) |>
  layer_dense(units = 512, activation = "relu") %>%
  layer_dropout(rate = 0.5) |>
 # layer_dense(units = 512, activation = "relu") %>
  layer_dense(units = 512, activation = "relu") %>%
  layer_dropout(rate = 0.9) |>
#  layer_dense(units = 258, activation = "relu") %>%
#  layer_dropout(rate = 0.9) |>
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)

## store training in a history object so that we can see how the model is doing, also on a small validation set.
## validation split, means that 20% of the training data is reserved for validation.
history <- model %>% 
  fit(training_data, 
      training_labels_class,
      epochs = 20,
      # Suppress logging.
      verbose = 2,
  # Calculate validation results on 20% of the training data.
  validation_split = 0.2
)

## we can plot the history
plot(history)

## evaluate our model on thetest data
model %>% evaluate(test_data, test_labels_class, verbose = 2)

# Confusion matrix
pred <- model %>% predict(test_data, batch_size = 10)
y_pred = round(pred)
confusion_matrix = table(y_pred, test_labels_class)
confusion_matrix
```

So it seems that using the ChemBERT embeddings did not improve our model for the toxicity classification (binary) task. Let's see how ChemBERT is doing on the regression task.

## Prediction the LD50_LM with the ChemBERT embeddings
```{r}
set.seed(123)
## simpler models
model <- keras_model_sequential(input_shape = c(1*768)) |>
  layer_dense(units = 512, activation = "relu") %>%
#  layer_dropout(rate = 0.1) |>
  layer_dense(units = 512, activation = "relu") %>%
  layer_dropout(rate = 0.9) |>
 # layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 1)

model %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("mae")
)

history <- model %>% 
  fit(training_data, 
      training_labels_ld50_lm,
      epochs = 10,
      # Suppress logging.
      verbose = 2,
  # Calculate validation results on 20% of the training data.
  validation_split = 0.2
)

plot(history)

model %>% evaluate(test_data,  test_labels_ld50_lm, verbose = 2)
model |> predict(test_data) -> predictions

predictions[1,]
realvalue <- test_labels_ld50_lm[1]
realvalue

## lets plot the correlations
predictions <- tibble(
  .pred = predictions,
  .real = test_labels_ld50_lm
)

origin <- tibble(
  x = seq(-3, 5, length.out = 9),
  y = seq(-3, 5, length.out = 9)
)

predictions |>
  ggplot(aes(x = .real, y = .pred)) +
  geom_point(shape = 1) +
  geom_line(data = origin, aes(x = x, y = y), colour = "darkred")
```

## Classic machine learning approach with reggression trees 
It seems the ChemBERT embeddings did not improve our predictions (for now). How can we then make use of this new way of encoding. 
First, we can use the ChemBERT embedding to create a more robust simulirity matrix, that can benefit read accross. We preformed 
We will demonstrate this with an example where we combine structural embeddings created with ChemBERT and a Natural Language Based biological enrichment, to perform read across. This concept is also presented on a poster during the ESTIV conference (poster: Abstract #492)

Another possibility is to use these new embeddings to do a different, more classical modelling approach. Maybe bringing out the big gun of Deep Learning is not always the best option. Here we will explore if an XGBoost model, which is a variant of the regression tree models (such as Random Forest), can bring a better model to the table. 

We will build the model, using the `{tidymodels}` approach. This approach consists of the following steps

 - Split the data
 - Define a models recipe
 - Define a recipe for data pre-processing
 - Combine the recipes in a workflow
 - Fit the model
 - Evaluate the model
 - Tune the model using a grid to find the optimal hyperparameters
 - Refit the optimized model
 - Evaluate
 - Run predictions

```{r}
library(tidymodels)
set.seed(123)
## split the data
chembert_xgb <- chembert |>
  dplyr::select(-c(ld50_lm, dtxsid)) |> 
  mutate(class = as_factor(class))
  
  
chembert_split <- initial_split(data = chembert_xgb, 
                             prop = 0.80, 
                            strata = class)
chembert_train <- training(chembert_split)
chembert_test <- testing(chembert_split)


## prepare model recipe
# we use update_role() to define new roles for the class and id variables
xgb_mod <- boost_tree(mtry = 50, trees = 500) %>% 
  set_engine("xgboost") %>%
  set_mode("classification")

xgb_rec <- recipe(class ~ ., data = chembert_train) |>

 # step_center(all_numeric_predictors()) |>
#  step_scale(all_numeric_predictors()) |> 
  step_zv(all_predictors())

xgb_wf <- workflow() %>% 
  add_model(xgb_mod) %>% 
  add_recipe(xgb_rec)

#prep <- prep(rf_rec)
#juiced <- juice(prep)

set.seed(1)

## fit model
xgb_fit <- xgb_wf %>% 
  fit(data = chembert_train)

## see model metrics

xgb_fit %>% extract_fit_parsnip()
predict(xgb_fit, chembert_test)


## Model eval
xgb_fit %>% 
  predict( new_data = chembert_test) %>% 
  bind_cols(chembert_test["class"]) %>% 
  accuracy(truth= as.factor(class), .pred_class) 

## confusion matrix
caret::confusionMatrix(
  as.factor(chembert_test$class), 
  predict(xgb_fit, new_data = chembert_test)$.pred_class)

bind_cols(
    predict(xgb_fit, chembert_test),
    predict(xgb_fit, chembert_test, type = "prob"),
    chembert_test[,1]
  ) -> predictions
predictions
```

In stead of defining a value for the hyperparameters, we use the `tune()` function to act as a placeholder for the actual values from the tune grid:
```{r}
xgb_mod_tune <- boost_tree(
  mtry = tune(), 
  trees = tune(),
  tree_depth = tune()) %>% 
  set_engine("xgboost") %>%
  set_mode("classification")
```

In order to get a structured collection of possible combinations of our hyperperparameters, we can use the `grid_regular()` function. 
```{r}
tree_grid <- grid_regular(
  trees(),
  tree_depth(),
  finalize(mtry(), select(chembert_xgb , -class)),
  levels = 3)
tree_grid
```

Armed with this grid we need to create multiple folds of our data to run the models.
```{r}
set.seed(234)
cell_folds <- vfold_cv(chembert_train, v = 3)
```

We tune our parameters according the grid, over the data-folds we created. This step takes a long time to compute. I ran this on a 30 core VM, with 472 Gb RAM. A very large machine, in comparison to standard laptops, over 4x more compute power easily. On that machine it took about 2 hours for the code below to finish. That is why I stored the results on disk, so that you do not need to run this. For safety reasons, I out-commented the code, so that you do not accidentally run it. 
```{r, eval=FALSE}
# set.seed(345)
# 
# tree_grid <- grid_regular(
#    trees(),
#    tree_depth(),
#    finalize(mtry(), select(chembert_xgb , -class)),
#    levels = 3)
#  
#  
#  set.seed(234)
#  cell_folds <- vfold_cv(chembert_train, v = 3)
#  
# 
# xgb_wf_tune <- workflow() %>%
#   add_model(xgb_mod_tune) %>%
#   add_formula(class ~ .)
# 
# xgb_res <- 
#   xgb_wf_tune %>% 
#   tune_grid(
#     resamples = cell_folds,
#     grid = tree_grid, control = control_grid(verbose = TRUE))
#     
# 
# xgb_res
## write to disk
# readr::write_rds(xgb_res, file = here::here("data", "xgb_res_chembert.rds"))
```

## Read results from disk
```{r}
xgb_res <- readr::read_rds(here::here("data", "xgb_res_chembert.rds"))
xgb_res

hyper_p <- xgb_res |>
  collect_metrics()

xgb_res %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(trees, mean, color = tree_depth)) +
#  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0) +
  facet_wrap(~mtry)

xgb_res %>%
  show_best("accuracy")

best_boost <- xgb_res %>%
  select_best("accuracy")

final_wf <- 
  xgb_wf %>% 
  finalize_workflow(best_boost)
final_wf

final_fit <- 
  final_wf %>%
  last_fit(chembert_split) 

final_fit$.metrics
final_fit$.predictions

final_fit$.predictions[[1]] %>% 
  accuracy(truth= as.factor(class), .pred_class) 

caret::confusionMatrix(
  final_fit$.predictions[[1]]$.pred_class, final_fit$.predictions[[1]]$class )

```

## Bigger grid
https://cran.r-project.org/web/packages/doFuture/vignettes/doFuture.html
We can tune an even bigger grid to get a more fine tuned hyperparameter optimization.
Again, this will take a lot of time to run on a standard computer.
```{r, eval=FALSE}
# ## we can optimize the big grid tuning using a different engine
#  xgb_l_tune <- boost_tree(
#    trees = tune(),
#    mtry = tune(),
#    tree_depth = tune()) %>% 
#    set_engine("xgboost") |>
#    set_mode("classification")
# # 
# # tree_grid <- grid_regular(
# #   trees(),
# #   tree_depth(),
# #   finalize(mtry(), select(chember , -class)),
# #   levels = 10)
# # 
# # 
# # set.seed(234)
# # cell_folds <- vfold_cv(qsar_train, v = 10)
# # 
# 
# # 
# # set.seed(345)
# 
# ## read results from disk
# xgb_l_res <- readr::read_rds("xgb_l_res.rds")
# 
#  xgb_wf_l_tune <- workflow() %>%
#    add_model(xgb_l_tune) %>%
#    add_formula(class ~ .)
# 
# #xgb_l_res <- 
# #  xgb_wf_l_tune %>% 
# #  tune_grid(
# #    resamples = cell_folds,
# #    grid = tree_grid, control = control_grid(verbose = TRUE))
#     
# #xgb_l_res
# #readr::write_rds(xgb_l_res, "xgb_l_res.rds")
# 
# hyper_p <- xgb_l_res |>
#   collect_metrics()
# 
# xgb_l_res %>%
#   collect_metrics() %>%
#   mutate(tree_depth = factor(tree_depth)) %>%
#   ggplot(aes(trees, mean, color = tree_depth)) +
# #  geom_line(size = 1.5, alpha = 0.6) +
#   geom_point(size = 2) +
#   facet_wrap(~ .metric, scales = "free", nrow = 2) +
#   scale_x_log10(labels = scales::label_number()) +
#   scale_color_viridis_d(option = "plasma", begin = .9, end = 0) +
#   facet_wrap(~mtry)
# 
# xgb_l_res %>%
#   show_best("accuracy")
# 
# best_boost <- xgb_l_res %>%
#   select_best("accuracy")
# 
# final_wf <- 
#   xgb_wf %>% 
#   finalize_workflow(best_boost)
# final_wf
# 
# final_fit <- 
#   final_wf %>%
#   last_fit(qsar_split) 
# 
# final_fit$.metrics
# 
# xgb_fit %>% extract_fit_parsnip()
# #predict(final_fit, qsar_test)
# 
# final_fit$.predictions
# 
# final_fit$.predictions[[1]] %>% 
#   accuracy(truth= as.factor(class), .pred_class) 
# 
# caret::confusionMatrix(
#   final_fit$.predictions[[1]]$.pred_class, final_fit$.predictions[[1]]$class )

```

## Running a different model - Support Vector Machines
```{r}
library(tidymodels)
set.seed(123)

## prepare model recipe
# we use update_role() to define new roles for the class and id variables
svm_mod <- svm_rbf(cost = 5) %>% 
  set_engine("kernlab") %>%
  set_mode("classification")

svm_rec <- recipe(class ~ ., data = chembert_train) |>

 # step_center(all_numeric_predictors()) |>
#  step_scale(all_numeric_predictors()) |> 
  step_zv(all_predictors())

svm_wf <- workflow() %>% 
  add_model(svm_mod) %>% 
  add_recipe(svm_rec)

#prep <- prep(rf_rec)
#juiced <- juice(prep)

set.seed(1)

## fit model
svm_fit <- svm_wf %>% 
  fit(data = chembert_train)

## see model metrics

svm_fit %>% extract_fit_parsnip()
predict(svm_fit, chembert_test)


## Model eval
svm_fit %>% 
  predict( new_data = chembert_test) %>% 
  bind_cols(chembert_test["class"]) %>% 
  accuracy(truth= as.factor(class), .pred_class) 

## confusion matrix
caret::confusionMatrix(
  as.factor(chembert_test$class), 
  predict(xgb_fit, new_data = chembert_test)$.pred_class)

bind_cols(
    predict(xgb_fit, chembert_test),
    predict(xgb_fit, chembert_test, type = "prob"),
    chembert_test[,1]
  ) -> predictions
predictions
```

```{r}
## using fingerprints
set.seed(123)

fp_svm <- fp_tbl |>
  dplyr::select(-c(ld50_lm)) |> 
  mutate(class = as_factor(class))
  
fp_split <- initial_split(
  data = fp_svm, 
  prop = 0.80, 
  strata = class)
fp_train <- training(fp_split)
fp_test <- testing(fp_split)


svm_rec <- recipe(class ~ ., data = fp_train) |>

 # step_center(all_numeric_predictors()) |>
#  step_scale(all_numeric_predictors()) |> 
  step_zv(all_predictors())

svm_wf <- workflow() %>% 
  add_model(svm_mod) %>% 
  add_recipe(svm_rec)

set.seed(1)

## fit model
svm_fit <- svm_wf %>% 
  fit(data = fp_train)

## see model metrics
svm_fit %>% extract_fit_parsnip()
predict(svm_fit, fp_test)

## Model eval
svm_fit %>% 
  predict( new_data = fp_test) %>% 
  bind_cols(fp_test["class"]) %>% 
  accuracy(truth= as.factor(class), .pred_class) 

## confusion matrix
caret::confusionMatrix(
  as.factor(fp_test$class), 
  predict(svm_fit, new_data = fp_test)$.pred_class)

bind_cols(
    predict(svm_fit, fp_test),
    predict(svm_fit, fp_test, type = "prob"),
    fp_test[,1]
  ) -> predictions
predictions

```

## What's next?
We could take several steps to try and improve our models. Here, I focus on the deep learning part:

 1. Use different embeddings: e.g. Graph embeddings, see https://github.com/deepchem/deepchem/blob/master/examples/tutorials/Introduction_to_Graph_Convolutions.ipynb
 2. Extend the validation, using k-fold validation: see e.g Deep Learning with R, page 79
 3. Use different architecture
 4. Collect more training data
 5. Build an ensemble model (combining multiple models), see e.g. https://pubs.acs.org/doi/abs/10.1021/acs.chemrestox.9b00259
 6. Combine the Read-Across and Deep Learning approaches
 7. Expand the feature space to biological information
 
